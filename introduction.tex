\section{Introduction}
Many optimizations, whether manual or automatic,
benefit from performance models.
Static performance models such as IACA lessen programmers
the burden of isolating performance kernels from
the rest of the applications,
and of properly implementing micro-benchmark. Many compiler optimizations
use cost models of one form or another:
a typical automatic vectorizer, for instance,
use one to estimate the cost of vector packing
in the presence of irregular memory accesses~\cite{goslp};
instruction schedulers uses one to model the latency
and throughput of instructions~\cite{llvm-sched,gcc-sched}.

Cost models however often take a back seat to “actual” optimizations,
despite the accuracy of former ensures the effectiveness of latter.
For instance, Mendis et al.\cite{goslp}
recently published a technique to perform optimal\footnote{
Witch certain caveat.
In particular their formulation is optimal
only when the target machine only has vector-width of 2}
SLP Vectorization\cite{slp} that
incurred slowdown in some instances due to the inaccuracy of 
LLVM's cost models.
Without a comprehensive benchmark to pinpoint the weakness of their models,
developers cannot systematically improve these cost models;
model parameters
such as the throughput of individual instructions
are sometimes chosen haphazardly;
consider following quote taken from a commit message from LLVM\cite{llvm}
regarding its cost model\footnote{
https://reviews.llvm.org/D46276.
}:
\begin{quote}
X86's fix to avoid SDIV/UDIV vectorization is clumsy
(multiplying the vector costs x20)
and appears to have been implemented before the cost of
scalarization was being realistically accounted for...
\end{quote}

While there have been multiple efforts~\cite{agner,uops,exegesis} to measure per-instruction
statistics for x86-64 -- for performance model tuning and optimization --
there has been no analogous dataset for (or beyond) basic block throughput,
which cannot be computed directly given even perfect modelling of individual instructions,
due to microarchitectural optimizations such as out-of-order executions and macro-op fusion.
One of the challenges of curating and profiling a set of basic blocks diverse enough to
uncover the intricacies of modern x86-64 processors is automatically profiling arbitrary
basic blocks.
This is much more than simply measuring the latency of an unrolled basic block
since code snippets with memory accesses (and most do)
generally cannot be executed safely
outside of the context of their source applications.

We describe a benchmark for validating performance models
of x86 basic blocks.
In this work we focus on throughput prediction.
We make the following contributions:
\begin{itemize}
\item We describe a fully automatic technique
for measuring the throughput arbitrary of basic blocks extracted 
from raw binaries.
We have used this technique to profile the throughput of more than 2 million basic blocks.

\item We collected basic blocks from a wide range of domains.
We automatically classified the basic blocks
by their usage of different execution ports.
This classification allows developers to pinpoint weakness
of their performance models.
Maintainers of an automatic vectorizer, for instance,
can use this to fine-tune cost models
of vectorized basic blocks.

\item We evaluated four existing cost models:
IACA, llvm-mca -- 
which exposes LLVM’s internal optimization cost models,
OSACA\cite{osaca}, and Ithemal\cite{ithemal}.
We breakdown the strength and weakness of these cost models on
different classes of basic blocks;
e.g. we show that all such tools
have average error higher than 30\% 
when used to analyze numerical kernels running on Haswell machines.

\end{itemize}