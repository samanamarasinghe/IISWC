\section{Background}

\subsection{Existing Performance Models}
\label{sec:perf-models}

IACA~\cite{iaca} (Intel Architecture Code Analyzer) is a static analyzer
developed by Intel. It has support for Intel micro-architectures
since Nehalem.
Given a machine code snippet, IACA estimates the average number
of cycles it costs to execute the given code in an infinite 
loop.
Unlike its alternatives, IACA takes advantage of its knowledge
of Intel's proprietary processor optimization---such as zero-idioms and micro-op fusions---to make better 
predictions.

llvm-mca~\cite{llvm-mca} is a similar tool inspired by IACA. 
Implemented as an out-of-order super-scalar simulator,
it uses parameters (e.g., instruction throughput)
supplied by LLVM\cite{llvm}'s backend scheduling model.
Reusing of the scheduling model is an explicit design choice
made to expose LLVM's cost model for testing.
Thus the accuracy of llvm-mca has a bearing on
that of LLVM's scheduling model.

OSACA\cite{osaca} is an analyzer developed as the open-source
alternative to IACA. It is similar to llvm-mca in that
it is implemented as a parametrized out-of-order simulator---in
this case, the parameters come from the measured throughput
and latency data for individual instructions.

Ithemal\cite{ithemal} is a basic block throughput predictor
implemented as a deep neural network. Unlike other models discussed so far, 
tools, Ithemal is not a simulator in the sense that it outputs
a single throughput prediction for each input basic block
without reporting an interpretable execution trace.

Production compilers such as LLVM\cite{llvm} and GCC typically use cost models 
to guide optimizations.
Unlike ``user-facing'' models such as IACA or llvm-mca, these cost models typically
model the costs at the instruction level, rather than at whole basic block level.
These compilers also use multiple cost models.
LLVM, for instance uses (at least) three cost models: 
a generic, per-instruction IR cost model in its middle-end IR~\cite{llvm-cost};
one that captures the information required for instruction scheduling,
(i.e. the scheduling model~\cite{llvm-sched}, also used by llvm-mca);
and another one that models the cost of register uses---such as cost of loads
and register-to-register transfer---in register allocation~\cite{llvm-reg}.
GCC similarly employs analogous models~\cite{gcc-cost,gcc-sched}.
Out of the models discussed so far, to our best knowledge, 
the scheduling model of LLVM is the only one exposed by an interface
(in this case, llvm-mca) for testing in isolation from its client optimizations.
% TODO: can we make a stronger statement saying given our work, we think
% other models should be exposed as well?


\vspace{1em}
\noindent{\bf Modeling Assumptions.}
All of cost models discussed above make the following assumptions about the execution context
of the code they are modeling
\begin{itemize}
    \item All memory accesses hit the L1-cache.
    \item All instructions reside in the instruction cache.
    \item The execution is unaffected by context switches and interrupts.
\end{itemize}

\subsection{Existing Validation Tools and Profilers}
We briefly discuss existing tools and sources used by developers to validate
processor performance  models.
There are roughly two categories of these sources:
\begin{enumerate*}
\item per-instruction cost table tabulated either by
consulting the manual or manual benchmarking;
\item machine code profiler for running microbenchmarks.
\end{enumerate*}

\textbf{Per-instruction Latency/Throughput Tables}
These datasets are created either by scraping hardware manual or
by manual profiling 
(to obtain the real instruction latency or throughput).
Current sources of per-instruction information are incomplete and sometimes inaccurate~\cite{uops}. 
More importantly, they do not lead directly to validating performance model at basic block level or more.
Although in general, it is possible to estimate the throughput of 
a basic block by inferring the scheduling of its instructions based on their
port usage---this is the approach taken by llvm-mca~\cite{llvm-mca} and OSACA\cite{osaca}---this
it is incomplete and does not take microarchitectural
optimizations that could alter the ``normal'' execution paths into account. 
Examples of these optimizations include micro-op fusion and zero-idioms.
Therefore, IACA\cite{iaca} is generally recognized as the more accurate analyzer
compared to its alternatives since it can exploit
private optimizations employed by Intel's processors.

Intel's manual\cite{intel-manual} provides the throughput and latency
of frequently used instructions.
Agner Fog\cite{agner} similarly provides a table of instruction throughput and latency.
Google's EXEgesis team~\cite{exegesis} provides a tool that can automatically extract
machine-readable instruction specifications from Intel's manual.
llvm-exegesis, designed by the same team, is a tool that determines
the latency of an input instruction opcode\footnote{
Currently the tool is limited to instructions that does not touch memory or floating point registers} 
by automatically generating a micro-benchmark that measures the opcode's average latency;
some LLVM's developers use the tool to validate LLVM's scheduling model.

Abel and Reineke\cite{uops} recently published their methodology
to reverse engineer the detailed mapping from each instruction to the
combination of execution ports
that these micro-ops can use.
They build instruction-to-port mapping using automatically generated micro-benchmarks.
Based on this mapping, they infer the steady-state scheduling of constituent
micro-ops for an instruction and calculate the instruction throughput based on
this inferred schedule.

\textbf{Machine Code Profiler} These tools allow users to perform
detailed microbenchmarking and are used by compiler writers manually
to verify compiler cost models.
These profilers are, however, unsuitable for our work---i.e., profiling
a large set of arbitrary basic blocks for systematic validation.
They require user intervention to profile arbitrary basic blocks. Most notably,
user annotation is needed to allocate memory and
initialize memory-addressing registers to prevent crashing.

Agner Fog~\cite{agner} provides a script to profile small code snippets.
The script reports the number of cycles as well as performance statistics such as 
the number of cache misses.
The script assumes that it is the user's responsibility to ensure
the successful execution of the benchmark;
for instance, the user is expected to allocate memory and initialize pointers.

nanoBench~\cite{nanobench} is a profiler similar to that of Agner Fog's~\cite{agner},
with two notable improvements.
It allows the user to specify which processor-specific hardware
to measure---in addition to cycle-counter. It also supports profiling in kernel-mode,
removing potential noise due to context-switches and interrupts.

% TODO:
% 1. talk about compiler cost models (both LLVM and GCC), mention that there is a chicken and egg issues
% 2. talk about stuff like agner fog's script